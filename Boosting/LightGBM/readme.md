# LightGBM介绍

**LightGBM**(**Light Gradient Boosting Machine**)是微软的开源分布式高性能Gradient Boosting框架，使用基于决策树的学习算法。

### LightGBM涉及到的优化([官方文档翻译](https://github.com/Microsoft/LightGBM/blob/master/docs/Features.rst))

* **速度、内存的优化**

    许多提升工具使用基于预排序的算法(近似直方图)（例如xgboost中的默认算法）来进行决策树学习。这是一个简单的解决方案，但不容易优化。LightGBM使用基于直方图的算法，它将连续特征（属性）值存储到离散区间。这加快了训练速度并减少了内存使用量。

   * **基于直方图的算法的优点**

      + **降低计算每次拆分增益的成本**
      
      基于预排序的算法具有时间复杂性 O(#data)，计算直方图具有时间复杂度O(#data)，但这仅涉及快速的总结操作。构建直方图后，基于直方图的算法具有时间复杂度O(#bins)，并且#bins远小于#data。
      
      + **使用直方图减法进一步加速**
      
      要在二叉树中获取一个叶子的直方图，请使用其父级及其邻居的直方图减法因此，它需要仅为一个叶子构建直方图（小于#data其邻居）。然后它可以通过直方图减法获得其邻居的直方图，成本较低（O(#bins)）
      
      + **减少内存使用量**
      
       用离散箱替换连续值。如果#bins很小，可以使用小数据类型，例如uint8_t来存储训练数据，无需存储用于预排序特征值的其他信息
       
      + **降低并行学习的通信成本**
      
* **稀疏优化**

     只需要O(2 * #non_zero_data)为稀疏特征构造直方图
     
* **精度优化**

    + **叶子（最好的）树木生长**
     
    大多数决策树学习算法按级别（深度）方向生成树，如下图所示：

   ![image](level_wise.png)

LightGBM以叶子方式生长树木（最佳优先）。它将选择具有最大增量损失的叶子来生长。保持#leaf固定的叶子算法往往比水平算法实现更低的损失。

叶子方式可能会导致过度拟合#data，因此LightGBM包含max_depth限制树深度的参数。然而，即使max_depth指定了树木，树木仍然会逐渐生长。

   ![image](leaf_wise.png)

   + **分类特征的最优分割**
  
通常使用单热编码来表示分类特征，但这种方法对于树学习者来说是次优的。特别是对于高基数分类特征，基于单热特征构建的树往往是不平衡的，并且需要非常深地生长以实现良好的准确性。

最佳解决方案是通过将其类别划分为2个子集来分割分类特征，而不是单热编码。如果要素具有k类别，则2^(k-1) - 1可能存在分区。但是回归树有一个有效的解决方案[8]。它需要O(k * log(k))找到最佳分区。

基本思想是根据每次拆分的培训目标对类别进行排序。更具体地说，LightGBM根据其累积值（sum_gradient / sum_hessian）对直方图（对于分类特征）进行排序，然后在排序的直方图上找到最佳分割。

* **网络通信的优化**

在LightGBM的并行学习中，它只需要使用一些集体通信算法，如“All reduce”，“All gather”和“Reduce scatter”。LightGBM实现了最先进的算法[9]。这些集体通信算法可以提供比点对点通信更好的性能。

* **并行学习中的优化**

LightGBM提供以下并行学习算法。

   * **特征并行**
   
  传统算法
并行特征旨在并行化决策树中的“查找最佳拆分”。传统特征并行的过程是：

垂直分区数据（不同的机器具有不同的功能集）。
工作人员在本地功能集上找到本地最佳分割点{feature，threshold}。
彼此沟通本地最好的分裂，并获得最好的分裂。
具有最佳拆分的工作人员执行拆分，然后将拆分的数据结果发送给其他工作人员。
其他工作人员根据收到的数据分割数据。

传统特征并行的缺点：

有计算开销，因为它无法加速时间复杂度的“分裂” O(#data)。因此，并行特征在#data大时不能很好地加速。
需要分割结果的通信，其成本约为O(#data / 8)（一个数据一位）。
LightGBM中的并行特征
由于特征并行在#data大的时候不能很好地加速，我们做了一点改变：不是垂直分割数据，而是每个工人都拥有完整的数据。因此，LightGBM不需要为分割数据结果进行通信，因为每个工作人员都知道如何分割数据。并且#data不会更大，因此在每台机器中保存完整数据是合理的。

LightGBM中并行功能的过程：

工作人员在本地功能集上找到本地最佳分割点{feature，threshold}。
彼此沟通本地最好的分裂，并获得最好的分裂。
执行最佳分割。
但是，这个特征并行算法仍然会受到“分裂”时计算开销的影响#data。因此，当#data大的时候使用并行数据会更好。

   * **数据并行**
   
传统算法
数据并行旨在并行化整个决策学习。数据并行的过程是：

水平分区数据。
工人使用本地数据构建局部直方图。
合并来自所有局部直方图的全局直方图。
从合并的全局直方图中找到最佳拆分，然后执行拆分。
传统数据并行的缺点：

通信成本高。如果使用点对点通信算法，则一台机器的通信成本约为O(#machine * #feature * #bin)。如果使用集体通信算法（例如“All Reduce”），通信成本约为O(2 * #feature * #bin)（在[9]的第4.5章中检查“All Reduce”的成本）。
LightGBM中的数据并行
我们降低了LightGBM中并行数据的通信成本：

LightGBM使用“Reduce Scatter”来合并不同工人的不同（非重叠）特征的直方图，而不是“从所有局部直方图中合并全局直方图”。然后，工作人员在本地合并直方图上找到本地最佳分割，并同步全局最佳分割。
如前所述，LightGBM使用直方图减法来加速训练。基于此，我们只能为一片叶子传达直方图，并通过减法得到其邻居的直方图。
考虑到所有因素，LightGBM中的数据并行具有时间复杂性O(0.5 * #feature * #bin)。

  * **投票并行**
  
  投票并行进一步降低了数据并行中的通信成本，使其成本不变。它使用两阶段投票来降低特征直方图的通信成本[10]。

* **GPU支持**



* **应用程序和指标**

LightGBM支持以下应用程序：

   * **回归，目标函数是L2损失**
   * **二进制分类，目标函数是logloss**
   * **多分类**
   * **交叉熵，目标函数是logloss，支持非二进制标签的训练**
   * **lambdarank，目标函数是使用NDCG的lambdarank**
   
LightGBM支持以下指标：

  * **L1损失**
  * **L2损失**
  * **记录丢失**
  * **分类错误率**
  * **AUC**
  * **NDCG**
  * **地图**
  * **多级日志丢失**
  * **多级错误率**
  * **公平**
  * **胡伯**
  * **泊松**
  * **位数**
  * **MAPE**
  * **库勒巴克-莱布勒**
  * **伽玛**
  * **特威迪**


其他特性
  * **max_depth树的限制，同时生长树叶**
  * **镖**
  * **L1 / L2正则化**
  * **套袋**
  * **列（特征）子样本**
  * **继续列车输入GBDT模型**
  * **继续训练输入得分文件**
  * **加权培训**
  * **培训期间的验证度量输出**
  * **多验证数据**
  * **多指标**
  * **提前停止（训练和预测）**
  * **叶指数的预测**
