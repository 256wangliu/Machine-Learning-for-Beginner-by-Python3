# CatBoost介绍

**CatBoost**是俄罗斯的搜索巨头Yandex在2017年开源的机器学习库，是**Gradient Boosting**(**梯度提升**) + **Categorical Features**(**类别型特征**)。类似于LightGBM，也是基于梯度提升决策树的机器学习框架。[关于CatBoost参见](https://tech.yandex.com/catboost/)，[CatBoost算法参见](http://papers.nips.cc/paper/7898-catboost-unbiased-boosting-with-categorical-features.pdf)

### 一，CatBoost程序文件
### 二，CatBoost的优点(官宣)

1. 性能卓越：在性能方面可以匹敌任何先进的机器学习算法；

1. 鲁棒性/强健性：它减少了对很多超参数调优的需求，并降低了过度拟合的机会，这也使得模型变得更加具有通用性。

1. 易于使用：提供与scikit集成的Python接口，以及R和命令行界面；

1. 实用：可以处理类别型、数值型特征；

1. 可扩展：支持自定义损失函数；

### 三，CatBoost介绍

以下内容主要翻译自[论文](http://learningsys.org/nips17/assets/papers/paper_11.pdf)


### 论文题目：

**CatBoost: gradient boosting with categorical features support**

  CatBoost: 支持类别型特征梯度提升的库

##### 作者：

**Anna Veronika Dorogush, Vasily Ershov, Andrey Gulin**

##### 摘要：

本文介绍了一个新的开源的梯度提升库——CatBoost，它可以很好地处理类别型特征，并且在许多流行的公共数据集上，其性能超越了目前同样基于梯度提升的其他算法。这个库学习基于GPU实现，评分基于CPU实现，针对许多不同尺寸的数据样本集合，其速度都明显快于其他梯度提升库。

### 1 引言

梯度提升是一种强大的机器学习算法，在许多不同领域的应用中都能获得很好的结果。多年来，在解决具有异构特征、噪声和复杂依赖关系的数据的学习问题时，例如web搜索，推荐系统、天气预报以及其他方面的问题，它都作为首选方法。通过对函数空间中的梯度下降相对应的贪婪过程建模理论，可以解释如何通过迭代组合模型(弱预测器)来构建强预测器是可行的。

大多数流行的梯度提升算法利用决策树作为基本预测器。对于数值型特征使用决策树很方便，但是实际中，许多数据集包括类别型特征，这些特征对预测也很重要。类别型特征具有离散的值，并且这些值之间的比较(例如用户ID，城市名称)是没意义的，梯度提升算法中处理这类特征的最常用的方法就是在训练之前，也就是数据预处理阶段，将这些特征的值转换为数字。

本文提出了一种新的可以很好的处理类别型特征的梯度提升算法，并且该算法的改进之处就在于在训练的时候处理这些特征，而不是在数据预处理阶段。该算法的另一个优点是使用新的方法计算叶子节点的值来生成树，并且这种计算方式有助于减少过拟合。

在许多不同的流行数据集上的，本文算法的性能均优于目前最先进的梯度提升算法库GBDT，XGBoost，LightGBM以及H2O。这个算法命名为CatBoost（“分类提升”），源码已经开源。

CatBoost具有CPU和GPU实现。GPU实现使得学习更快，并且在很多不同大小的数据集上，速度都快于开源的GBDT的GPU实现，以及XGBoost和LightGBM。该库还具有快速的CPU评分，经过验证，其性能超过XGBoost和LightGBM。

### 2 类别型特征

类别型特征，也就是其值，也就是类别，是离散的特征，并且值之间的比较是没有意义的，因此这样的特征不能在二叉树中直接使用。通常的做法就是在数据预处理阶段将他们转化为数字。
 
对于类别数较少的类别型特征，常用的处理方式就是独热编码，也就是将原始的特征去除掉，然后将转码后的特征加到数据中。独热编码的工作在数据与处理阶段和学习阶段都可以执行，在CatBoost中实现了在学习阶段执行的一种更为有效的方式。

另一种处理类别型特征的方式就是利用样本的标签进行统计。对于给定的数据集D={(**X1**,**Y1**), (**X2**,**Y2**)，……，(**Xn**,**Yn**)}。其中**X**i=(xi1,xi2,……，xim)为m个特征的值的向量，其中一些特征是数值型的，令一些是类别型的。**Yi**表示数据样本的类别值。

最简单的方式就是计算整个数据集的平均标签值替换类别特征的值。

很明显，这个处理会导致过拟合。例如，如果类别型特征中的某个值只有一个，因此转换后的数值就等于这个样本的标签值。避免此问题的一个直接的做法就是将数据集划分为2部分，其中一部分用于计算转换的数字，另一部分用来学习。这种方式虽然降低了过拟合，但是同时减少了用于学习、计算的样本数量。

CatBoost利用了一种更为有效的策略，降低过拟合的同时也保证了全部数据集用于学习。也就是采用了数据集的一种随机排列，对于每个样本而言，对具有相同类别特征值的样本计算的平均标签值放在数据集排列的指定值之前。

其中P是先验项，先验项的权重a>0。添加先验项是一个普遍做法，针对类别数较少的特征，它可以减少噪音数据。对于回归问题，一般情况下，先验项可取数据集的均值。对于二分类，先验项是正例的先验概率。利用多个数据集排列也是有效的，但是，直接计算可能导致过拟合。CatBoost利用了一个比较新颖的计算叶子节点值的方法，这种方式可以避免多个数据集排列中直接计算会出现过拟合的问题，这将在下一节中讨论，

#### 2.1 特征组合

值得注意的是几个特征的组合都可视为新的特征。例如，假设任务是音乐推荐，并且我们有两个类别型特征：用户ID和音乐流派。
例如，有些用户更喜欢摇滚乐。根据（公式1），将用户ID和音乐流派转换为数字特征时，就会丢失这些信息。
结合这两个特征则可以解决这个问题，并且得到了一个新的强大特征。然而，组合的数量会随着数据集中类别型特征的数量成指数增长，
因此不可能在算法中考虑所有这些组合。为当前树构造新的分割点时，CatBoost会采用贪婪的策略考虑组合。
对于树的第一次分割，不考虑任何组合。对于下一个分割，CatBoost将当前树中存在的所有组合和类别型特征与数据集中的所有类别型特征相结合。
组合被动态地转换为数字。CatBoost还通过以下方式生成数值型特征和类别型特征的组合：树中选择的所有分割点都被视为具有两个值的类别，并且以与分类方法相同的方式组合使用


#### 2.2 重要的实现细节

用数字代替类别的另一种方法是计算该类别在数据集中的出现次数。这是一种简单但强大的技术，在CatBoost也有实现。这种统计量也是用于特征组合。

CatBoost算法为了在每个步骤中拟合最优的先验条件，我们考虑几个先验条件，为每个先验条件构造一个特征，这在质量上比上述标准技术更有效。

### 3 克服梯度偏差

CatBoost以及所有标准梯度提升算法都是构建新树来拟合当前模型的梯度。然而，所有经典的boosting算法都存在由有偏的点态梯度估计引起的过拟合问题。在每个步骤中使用的梯度都使用当前模型中的相同的数据点来估计。这导致估计梯度在特征空间的任何域中的分布与该域中梯度的真实分布相比会发生偏移，从而导致过拟合。有偏梯度的概念在以前的文献[1]，[9]中已经讨论过。我们在文[5]对这一问题进行了形式化分析，还对经典的梯度提升算法进行了改进来解决这一问题。CatBoost实现了这些改进之一，下面简要介绍：

在许多利用GBDT技术的算法（例如，XGBoost、LightGBM）中，构建下一棵树分为两个阶段：选择树结构和在树结构固定计算叶子节点的值。为了选择最佳的树结构，算法通过不同的分割枚举，用这些分割构建树，对得到的叶子节点中计算值，然后对树进行评分并选择最佳的分割。两个阶段的叶子节点的值都作为梯度[8]或牛顿步长的近似值来计算。在CatBoost中，第二阶段使用传统的GBDT方案执行，第一阶段使用修改后的版本。

根据我们在文[5]中的经验结果和理论分析，使用梯度步长的无偏估计是很关键的。设Fi为构建第i棵树后的模型，gi(Xk，Yk)为建立第i棵树后第k个训练样本上的梯度值。为了使梯度gi(Xk，Yk)无偏于模型Fi，我们需要在没有观测值Xk的情况下对Fi进行训练。由于我们需要对所有训练实例计算无偏的梯度估计，所以不能使用任何观测值来训练Fi，乍一看训练变得不可能。我们考虑以下技巧来处理这个问题：对于每个示例Xk，我们训练一个单独的模型Mk，该模型Mk从未使用此示例的梯度估计进行更新。使用Mk，我们估计Xk上的梯度，并使用这个估计对结果树进行评分。让我们呈现伪代码，解释如何执行此技巧。设Loss(y，a)为优化损失函数，其中y为标签值，a为公式值。

在CatBoost中，我们生成训练数据集的随机排列。为了增强算法的鲁棒性，我们使用了几种排列：我们对随机排列进行采样，并在其基础上获得梯度。这些排列与用于计算类别型特征的统计信息的排列相同。我们使用不同的排列来训练不同的模型，因此使用几个排列不会导致过拟合。对于每个排列，我们训练n个不同的模型Mi，如上所示。这意味着为了构建一棵树，针对每个排列需要存储并重新计算，其复杂度近似于O(n^2)：对于每个模型Mi，我们必须更新Mi(X1)，.…，Mi(Xi)。因此，这种操作的结果复杂度是O(sn^2)。在我们的实现中，我们使用一个重要的技巧，它将一个树结构的复杂性降低到O(sn)：对于每个排列，我们不存储和更新值Mi(Xj)，而是保持值M0i(Xj)，i=1，.…，[log2(n)]，j<2i+1，其中M0i(Xj)是基于前2i样本的样本j的近似值。然后，预测值M0i(Xj)的数目不大于P0≤i≤log2(n)2i+1<4n，在近似M0i(Xk)的基础上估计了用于选择树结构的示例Xk上的梯度，其中i=[log2(k)]。

### 4 快速评分

CatBoost使用对称树作为基本预测器。在这类树中，相同的分割准则在树的整个级别上使用[12，13]。这种树是平衡的，不太容易过拟合。梯度提升对称树被成功地用于各种学习任务[7，10]。在对称树中，每个叶子节点的索引可以被编码为长度等于树深度的二进制向量。这个事实在CatBoost模型评估器中得到了广泛的应用：我们首先将所有使用的浮点特征、统计信息和独热编码特征进行二值化，然后使用二进制特征来计算模型预测。

所有样本的所有二进制特征值都存储在连续向量B中。叶子节点的值存储在大小为2d的浮点向量中，其中d是树深度。为了计算第t棵树的叶子节点的索引，对于示例x，我们建立了一个二进制向量Pd_1i=02i·B(x，f(t，i))，其中B(x，f)是从向量B读取的示例x上的二进制特征f的值，而f(t，i)是从深度i上的第t树中的二进制特征的数目。

这些向量可以以数据并行方式构建，这种方式可以实现高达3倍的加速。这导致比所有现有的得分器快得多，正如我们的实验所示。

### 5 基于GPU实现快速学习

#### 5.1 密集的数值特征

One of the most important building blocks for any GBDT implementation is searching for the best split. This block is the main computation burden for building decision tree on dense numerical datasets. CatBoost uses oblivious decision trees as base learners and performs feature discretization into a fixed amount of bins to reduce memory usage [10]. The number of bins is the parameter of the algorithm. As a result we could use histogram-based approach for searching for best splits. Our approach to building decision trees on GPU is similar in spirit to one described in [11]. We group several numerical features in one 32-bit integer and currently use:

对于任何GBDT算法的实现而言，最重要的步骤就是搜索最佳分割。对于密集的数值特征数据集来说，该步骤是在建立决策树的主要计算负担。CatBoost使用遗忘的决策树作为基础学习者，并将特征离散化为固定数量的箱子以减少内存使用[10]。箱子的数量是算法的参数。因此，我们可以使用基于直方图的方法来搜索最佳分割。我们在GPU上构建决策树的方法在精神上类似于[11]中所描述的方法。我们将几个数字特征分组为一个32位整数，目前使用：


#### 5.2 类别型特征

CatBoost实现了几种处理类别型特征的方法。对于独热编码特征，我们不需要任何特殊处理，基于直方图的分割搜索方法可以很容易地应用于这种情况。在数据预处理阶段，也可以对单个类别型特征进行统计计算。CatBoost还对特征组合使用统计信息。处理它们是算法中速度最慢、消耗内存最多的部分。

我们使用完美哈希来存储类别特性的值以减少内存使用。由于GPU内存的限制，我们在CPU RAM中存储按位压缩的完美哈希，以及要求的数据流、重叠计算和内存操作。动态地构造特征组合要求我们为这个新特征动态地构建（完美）哈希函数，并为哈希的每个唯一值计算关于某些排列的统计数据。我们使用基数排序来构建完美的哈希，并通过哈希来分组观察。在每个组中，我们需要计算一些统计量的前缀和。该统计量的计算使用分段扫描GPU图元进行（CatBoost分段扫描实现通过算子变换[16]完成，并且基于CUB[6]中扫描图元的高效实现）。

#### 5.3 多GPU支持

CatBoost GPU实现支持几个现成的GPU。分布式树学习可以通过样本或特征进行并行化。CatBoost使用具有多个学习数据集置换的计算方案，并在训练期间计算分类特征的统计数据。因此，我们需要使用特征并行学习。



