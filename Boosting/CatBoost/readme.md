# CatBoost介绍

**CatBoost**是俄罗斯的搜索巨头Yandex在2017年开源的机器学习库，是**Gradient Boosting**(**梯度提升**) + **Categorical Features**(**类别型特征**)。类似于LightGBM，也是基于梯度提升决策树的机器学习框架。[详情参见](https://tech.yandex.com/catboost/)。

### 1，CatBoost介绍

以下内容主要翻译自[论文](http://learningsys.org/nips17/assets/papers/paper_11.pdf)


### 论文题目：

**CatBoost: gradient boosting with categorical features support(支持类别型特征梯度提升)**

##### 作者：

**Anna Veronika Dorogush, Vasily Ershov, Andrey Gulin**

##### 摘要：

本文介绍了一个新的开源的梯度提升库——CatBoost，它可以很好地处理类别型特征，并且在许多流行的公共数据集上，其性能超越了目前同样基于梯度提升的其他算法。这个库学习基于GPU实现，评分基于CPU实现，针对许多不同尺寸的数据样本集合，其速度都明显快于其他梯度提升库。

### 1 引言

梯度提升是一种强大的机器学习算法，在许多不同领域的应用中都能获得很好的结果。多年来，在解决具有异构特征、噪声和复杂依赖关系的数据的学习问题时，例如web搜索，推荐系统、天气预报以及其他方面的问题，它都作为首选方法。通过对函数空间中的梯度下降相对应的贪婪过程建模理论，可以解释如何通过迭代组合模型(弱预测器)来构建强预测器是可行的。

大多数流行的梯度提升算法利用决策树作为基本预测器。对于数值型特征使用决策树很方便，但是实际中，许多数据集包括类别型特征，这些特征对预测也很重要。类别型特征具有离散的值，并且这些值之间的比较(例如用户ID，城市名称)是没意义的，梯度提升算法中处理这类特征的最常用的方法就是在训练之前，也就是数据预处理阶段，将这些特征的值转换为数字。

本文提出了一种新的可以很好的处理类别型特征的梯度提升算法，并且该算法的改进之处就在于在训练的时候处理这些特征，而不是在数据预处理阶段。该算法的另一个优点是使用新的方法计算叶子节点的值来生成树，并且这种计算方式有助于减少过拟合。

在许多不同的流行数据集上的，本文算法的性能均优于目前最先进的梯度提升算法库GBDT，XGBoost，LightGBM以及H2O。这个算法命名为CatBoost（“分类提升”），源码已经开源。

CatBoost具有CPU和GPU实现。GPU实现使得学习更快，并且在很多不同大小的数据集上，速度都快于开源的GBDT的GPU实现，以及XGBoost和LightGBM。该库还具有快速的CPU评分，经过验证，其性能超过XGBoost和LightGBM。

### 2 类别型特征

类别型特征，也就是其值，也就是类别，是离散的特征，并且值之间的比较是没有意义的，因此这样的特征不能在二叉树中直接使用。通常的做法就是在数据预处理阶段将他们转化为数字。
 
对于类别数较少的类别型特征，常用的处理方式就是独热编码，也就是将原始的特征去除掉，然后将转码后的特征加到数据中。独热编码的工作在数据与处理阶段和学习阶段都可以执行，在CatBoost中实现了在学习阶段执行的一种更为有效的方式。

另一种处理类别型特征的方式就是利用样本的标签进行统计。对于给定的数据集D={(**X1**,**Y1**), (**X2**,**Y2**)，……，(**Xn**,**Yn**)}。其中**X**i=(xi1,xi2,……，xim)为m个特征的值的向量，其中一些特征是数值型的，令一些是类别型的。**Yi**表示数据样本的类别值。

最简单的方式就是计算整个数据集的平均标签值替换类别特征的值。

很明显，这个处理会导致过拟合。例如，如果类别型特征中的某个值只有一个，因此转换后的数值就等于这个样本的标签值。避免此问题的一个直接的做法就是将数据集划分为2部分，其中一部分用于计算转换的数字，另一部分用来学习。这种方式虽然降低了过拟合，但是同时减少了用于学习、计算的样本数量。

CatBoost利用了一种更为有效的策略，降低过拟合的同时也保证了全部数据集用于学习。也就是采用了数据集的一种随机排列，对于每个样本而言，对具有相同类别特征值的样本计算的平均标签值放在数据集排列的指定值之前。

其中P是先验项，先验项的权重a>0。添加先验项是一个普遍做法，针对类别数较少的特征，它可以减少噪音数据。对于回归问题，一般情况下，先验项可取数据集的均值。对于二分类，先验项是正例的先验概率。利用多个数据集排列也是有效的，但是，直接计算可能导致过拟合。CatBoost利用了一个比较新颖的计算叶子节点值的方法，这种方式可以避免多个数据集排列中直接计算会出现过拟合的问题，这将在下一节中讨论，

#### 2.1 特征组合

值得注意的是将几个分类特征的任何组合都可视为新的特征。例如，假设任务是音乐推荐，并且我们有两个分类特征：用户ID和音乐流派。例如，有些用户更喜欢摇滚乐。根据（公式1），将用户ID和音乐流派转换为数字特征时，就会丢失这些信息。结合两个特性则可以解决这个问题，并提供了一个新的强大特征。然而，组合的数量随着数据集中类别特征的数量成指数增长，并且不可能在算法中考虑所有这些特征。当为当前树构造新的分割时，CatBoost会采用贪婪策略考虑组合。对于树中的第一次拆分，不考虑任何组合。对于下一个分割，CatBoost将当前树中存在的所有组合和分类特征与数据集中的所有分类特征相结合。组合值被动态地转换为数字。CatBoost还通过以下方式生成数值特征和分类特征的组合：树中选择的所有分段都被视为具有两个值的分类，并用于
组合方式与分类方法相同。


#### 2，CatBoost的优点(官宣)

1. 性能卓越：在性能方面可以匹敌任何先进的机器学习算法；

1. 鲁棒性/强健性：它减少了对很多超参数调优的需求，并降低了过度拟合的机会，这也使得模型变得更加具有通用性。

1. 易于使用：提供与scikit集成的Python接口，以及R和命令行界面；

1. 实用：特征值可以为字符串或者数字，无需将字符串经过编码；

1. 可扩展：支持自定义损失函数；


### 2，CatBoost程序文件
